"""
RAG Chat Service using Azure OpenAI and Azure AI Search

This module implements a Retrieval Augmented Generation (RAG) service that connects
Azure OpenAI with Azure AI Search. RAG enhances LLM responses by grounding them in
your enterprise data stored in Azure AI Search.
"""
import logging
from typing import List
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from openai import AsyncAzureOpenAI
from app.models.chat_models import ChatMessage
from app.config import settings

logger = logging.getLogger(__name__)


class RagChatService:
    """
    Service that provides Retrieval Augmented Generation (RAG) capabilities
    by connecting Azure OpenAI with Azure AI Search for grounded responses.
    
    This service:
    1. Handles authentication to Azure services using Managed Identity
    2. Implements the "On Your Data" pattern using Azure AI Search as a data source
    3. Processes user queries and returns AI-generated responses grounded in your data
    """
    
    def __init__(self):
        """Initialize the RAG chat service using settings from app config"""
        # Store settings for easy access
        self.openai_endpoint = settings.azure_openai_endpoint
        self.gpt_deployment = settings.azure_openai_gpt_deployment
        self.embedding_deployment = settings.azure_openai_embedding_deployment
        self.search_url = settings.azure_search_service_url
        self.search_index_name = settings.azure_search_index_name
        self.system_prompt = settings.system_prompt
        
        # Create Azure credentials for managed identity
        # This allows secure, passwordless authentication to Azure services
        self.credential = DefaultAzureCredential()
        token_provider = get_bearer_token_provider(
            self.credential,
            "https://cognitiveservices.azure.com/.default"
        )
        
        # Create Azure OpenAI client
        # We use the latest Azure OpenAI Python SDK with async support
        self.openai_client = AsyncAzureOpenAI(
            azure_endpoint=self.openai_endpoint,
            azure_ad_token_provider=token_provider,
            api_version="2024-10-21"
        )
        
        logger.info("RagChatService initialized with environment variables")
    
    async def get_chat_completion(self, user_message: str = None, conversation_history: List[ChatMessage] = None):
        """
        Process a chat completion request with RAG capabilities by integrating with Azure AI Search
        
        This method:
        1. Formats the conversation history for Azure OpenAI
        2. Configures Azure AI Search as a data source using the "On Your Data" pattern
        3. Sends the request to Azure OpenAI with data_sources parameter
        4. Returns the response with citations to source documents
        
        Args:
            user_message: Current user message (optional, for direct message handling)
            conversation_history: List of chat messages from the conversation history (optional)
            
        Returns:
            Dict with message content and metadata from Azure AI Search
        """
        try:
            # Initialize messages with system prompt
            messages = [{
                "role": "system", 
                "content": self.system_prompt
            }]
            
            # Add conversation history if provided
            if conversation_history:
                # Limit chat history to the 20 most recent messages to prevent token limit issues
                recent_history = conversation_history[-20:] if len(conversation_history) > 20 else conversation_history
                
                # Add conversation history
                for msg in recent_history:
                    messages.append({
                        "role": msg.role,
                        "content": msg.content
                    })
            
            # Add current user message if provided
            if user_message:
                messages.append({
                    "role": "user",
                    "content": user_message
                })
            
            # Configure Azure AI Search data source according to the "On Your Data" pattern
            # This connects Azure OpenAI directly to your search index without needing to
            # manually implement vector search, chunking, or semantic rankers
            data_source = {
                "type": "azure_search",
                "parameters": {
                    "endpoint": self.search_url,
                    "index_name": self.search_index_name,
                    "authentication": {
                        "type": "system_assigned_managed_identity"
                    },
                    # Combines vector and traditional search
                    "query_type": "vector_semantic_hybrid",
                    # The naming pattern for semantic configuration is generated by Azure AI Search 
                    # during integrated vectorization and cannot be customized
                    "semantic_configuration": f"{self.search_index_name}-semantic-configuration",
                    "embedding_dependency": {
                        "type": "deployment_name",
                        "deployment_name": self.embedding_deployment
                    }
                }
            }
            
            # Call Azure OpenAI for completion with the data_sources parameter directly
            # The data_sources parameter enables the "On Your Data" pattern, where
            # Azure OpenAI automatically retrieves relevant documents from your search index
            response = await self.openai_client.chat.completions.create(
                model=self.gpt_deployment,
                messages=messages,
                extra_body={
                    "data_sources": [data_source]
                },
                stream=False
            )
            
            # Extract the message content and return formatted response
            if response.choices and len(response.choices) > 0:
                message_content = response.choices[0].message.content
                return {
                    "message": message_content,
                    "citations": getattr(response.choices[0].message, 'context', {}).get('citations', []) if hasattr(response.choices[0].message, 'context') else []
                }
            else:
                return {
                    "message": "No pude generar una respuesta.",
                    "citations": []
                }
            
        except Exception as e:
            logger.error(f"Error in get_chat_completion: {str(e)}")
            # Propagate all errors to the controller layer
            raise


# Create singleton instance
rag_chat_service = RagChatService()
